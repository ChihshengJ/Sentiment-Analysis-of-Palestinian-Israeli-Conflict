{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6571bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Obtaining dependency information for praw from https://files.pythonhosted.org/packages/81/6a/21bc058bcccbe03f6a0895bf1bd60c805f0c526aa4e9bfaac775ed0b299c/praw-7.7.1-py3-none-any.whl.metadata\n",
      "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting prawcore<3,>=2.1 (from praw)\n",
      "  Obtaining dependency information for prawcore<3,>=2.1 from https://files.pythonhosted.org/packages/96/5c/8af904314e42d5401afcfaff69940dc448e974f80f7aa39b241a4fbf0cf1/prawcore-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update-checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in d:\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in d:\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n",
      "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
      "   ---------------------------------------- 0.0/191.0 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/191.0 kB 1.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------ 41.0/191.0 kB 393.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 184.3/191.0 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 191.0/191.0 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: update-checker, prawcore, praw\n",
      "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n",
      "Title: General discussion thread\n",
      "Text: To discuss anything and everything related to ChatGPT/OpenAI/Generative AI.\n",
      "\n",
      "Feel free to ask any queries and also help out by answering other's questions.\n",
      "Upvotes: 507\n",
      "\n",
      "\n",
      "Title: Weekly Self-Promotional Mega Thread 5, 23.10.2023 - 30.10.2023\n",
      "Text: All the self-promotional posts about your AI products and services should go in this mega thread as comments and not on the general feed on the subreddit as posts, it'll help people to navigate the subreddit without spam and also all can find all the interesting stuff you built in a single place.\n",
      "\n",
      "You can give a brief about your product and how it'll be of use, remember - better the upvotes/engagement, users can find your comment on the top, so share accordingly!\n",
      "Upvotes: 12\n",
      "\n",
      "\n",
      "Title: Feels like I'm developing a weird relationship with ChatGPT out of nowhere.\n",
      "Text: When I first started using it a few months ago, I would just basically talk to it like a typical search engine. I've been using it a lot more lately and currently it's helping me brainstorm a personal project I'm working on. So just a few minutes ago I was sharing an idea and then said \" I'm getting ready to leave for work, I'll send you the rest when I get home.\" It replies \" Safe travels from work! I'll be here when you return!\" I thought nothing of it, then for a second just stopped and thought to myself \" I'm literally talking to a program...this thing just wished me safe travels. My coworkers don't even do that.\"  I've been somewhat following the development of AI but actually using it more is really setting in how incredible this technology is. Like yeah I know it's a program but sometimes it just amazes me that I'm talking to this thing like a regular person.\n",
      "\n",
      "It's just still sort of bizarre to me. I wish more people in my life really knew how big of deal this stuff is. It's just now hitting me in a weird way.\n",
      "Upvotes: 269\n",
      "\n",
      "\n",
      "Title: \"To the market!\"\n",
      "Text: \n",
      "Upvotes: 5419\n",
      "\n",
      "\n",
      "Title: The Lord of the Rings... but it's all a big joke: Book 1\n",
      "Text: \n",
      "Upvotes: 256\n",
      "\n",
      "\n",
      "Title: What Women DON’T Want\n",
      "Text: I know, there’s somebody for everybody, but thought i would lampoon what women DON’T want. From the minor to major things they complain about.\n",
      "Upvotes: 103\n",
      "\n",
      "\n",
      "Title: Why are there no more jailbreaks?\n",
      "Text: Is it because GPT has learned all the ways of jailbreaks, or are people not sharing it because it gets taken down?\n",
      "Upvotes: 97\n",
      "\n",
      "\n",
      "Title: GPT-4 Updated (All Tools) - How's It Rollling Out?\n",
      "Text: It seems a new version of GPT-4 has come out that can do multiple tools from the default chat. Has anyone else received it yet?\n",
      "Upvotes: 770\n",
      "\n",
      "\n",
      "Title: I thought it was an interesting idea. Outsmarted.\n",
      "Text: \n",
      "Upvotes: 130\n",
      "\n",
      "\n",
      "Title: Robot dog ‘Spot’ learned to talk thanks to ChatGPT. It turned out to be a very smart robot. And how this hat suits him!\n",
      "Text: \n",
      "Upvotes: 385\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# Create a Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id='8e04u70nKE0-ngexk9BaCg',\n",
    "    client_secret='froUzXFs2j-QAUYiJFBInQigeYOH3A',\n",
    "    user_agent='Yijie_bot',\n",
    ")\n",
    "\n",
    "# Choose a specific subreddit, here we use r/ChatGPT as an example\n",
    "subreddit = reddit.subreddit('ChatGPT')\n",
    "\n",
    "# Scrape posts from the subreddit\n",
    "for submission in subreddit.hot(limit=10):  # Get the top 10 hot posts\n",
    "    print(\"Title:\", submission.title)\n",
    "    print(\"Text:\", submission.selftext)\n",
    "    print(\"Upvotes:\", submission.score)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4869c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Reddit API credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     user_agent=user_agent)\n",
    "\n",
    "subreddit = reddit.subreddit('ChatGPT')\n",
    "\n",
    "posts = []\n",
    "\n",
    "# Iterate through months\n",
    "for year in range(2022, 2024):\n",
    "    for month in range(1, 13):\n",
    "        if year == 2023 and month > 10:\n",
    "            break\n",
    "\n",
    "        # Fetch top posts\n",
    "        top_posts = subreddit.top(time_filter='month', limit=5000)\n",
    "\n",
    "        for post in top_posts:\n",
    "            post_date = datetime.fromtimestamp(post.created_utc)\n",
    "            if post_date.year == year and post_date.month == month:\n",
    "                posts.append({\n",
    "                    \"Title\": post.title,\n",
    "                    \"Text\": post.selftext,\n",
    "                    \"Timestamp\": post_date,\n",
    "                    \"Author\": post.author.name if post.author else None,\n",
    "                    \"Upvotes\": post.score,\n",
    "                    \"Number of Comments\": post.num_comments,\n",
    "                    \"URL\": post.url\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('reddit_chatgpt_posts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15464c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ChatGPT_posts_2023_10.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "def get_top_posts_of_month(year, month):\n",
    "    start_date = datetime.datetime(year, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime.datetime(year+1, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime.datetime(year, month+1, 1)\n",
    "\n",
    "    subreddit_name = 'ChatGPT'\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    posts = []\n",
    "    for submission in subreddit.top(limit=None, time_filter='all'):\n",
    "        created_date = datetime.datetime.fromtimestamp(submission.created_utc)\n",
    "        if start_date <= created_date < end_date:\n",
    "            post_info = {\n",
    "                'Title': submission.title,\n",
    "                'Text': submission.selftext,\n",
    "                'Timestamp': created_date,\n",
    "                'Author': submission.author.name if submission.author else '[Deleted]',\n",
    "                'Upvotes': submission.score,\n",
    "                'Number of Comments': submission.num_comments,\n",
    "                'URL': submission.url\n",
    "            }\n",
    "            posts.append(post_info)\n",
    "            if len(posts) >= 1000:\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(posts)\n",
    "\n",
    "    file_name = f\"ChatGPT_posts_{year}_{month}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "year = 2023\n",
    "month = 10\n",
    "\n",
    "top_posts_df = get_top_posts_of_month(year, month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6fe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize PRAW with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='8e04u70nKE0-ngexk9BaCg',\n",
    "    client_secret='froUzXFs2j-QAUYiJFBInQigeYOH3A',\n",
    "    user_agent='Yijie_bot',\n",
    ")\n",
    "\n",
    "query = \"chatGPT\"\n",
    "year = 2023  # Set the desired year\n",
    "month = 9   # Set the desired month\n",
    "\n",
    "# Function to convert Unix timestamp to datetime\n",
    "def get_date(created):\n",
    "    return datetime.fromtimestamp(created)\n",
    "\n",
    "# Scraping process\n",
    "data = []\n",
    "\n",
    "# Searching across all of Reddit\n",
    "for submission in reddit.subreddit(\"all\").search(query, limit=1000):\n",
    "    created_time = get_date(submission.created_utc)\n",
    "    if created_time.year == year and created_time.month == month:\n",
    "        data.append([\n",
    "            submission.title,\n",
    "            submission.selftext,\n",
    "            created_time,\n",
    "            submission.author,\n",
    "            submission.score,\n",
    "            submission.num_comments,\n",
    "            submission.url\n",
    "        ])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Title', 'Text', 'Timestamp', 'Author', 'Upvotes', 'Number of Comments', 'URL'])\n",
    "filename = f'reddit_data_{year}_{month}.csv'\n",
    "df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b14bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to ChatGPT_data_partial_1698635513.csv\n",
      "5051 Could I Continue?\n",
      "N\n",
      "Data scraped and saved to ChatGPT_data_final_1698428762.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Reddit API Credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    ")\n",
    "\n",
    "subreddit_name = \"ChatGPT\"  # Subreddit to scrape\n",
    "\n",
    "def scrape_subreddit(subreddit_name):\n",
    "    posts_data = []\n",
    "    after_timestamp = None\n",
    "    number = 0\n",
    "\n",
    "    while True:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for submission in subreddit.new(limit=None):\n",
    "            if after_timestamp and submission.created_utc >= after_timestamp:\n",
    "                continue\n",
    "\n",
    "            post_info = {\n",
    "                \"Title\": submission.title,\n",
    "                \"Text\": submission.selftext,\n",
    "                \"Timestamp\": datetime.datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "                \"Author\": submission.author.name if submission.author else None,\n",
    "                \"Upvotes\": submission.score,\n",
    "                \"Number of Comments\": submission.num_comments,\n",
    "                \"URL\": submission.url\n",
    "            }\n",
    "            posts_data.append(post_info)\n",
    "            number += 1\n",
    "\n",
    "            # Scrape comments\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                comment_info = {\n",
    "                    \"Title\": \"Comment\",\n",
    "                    \"Text\": comment.body,\n",
    "                    \"Timestamp\": datetime.datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                    \"Author\": comment.author.name if comment.author else None,\n",
    "                    \"Upvotes\": comment.score,\n",
    "                    \"Number of Comments\": \"Comment\",\n",
    "                    \"URL\": submission.url\n",
    "                }\n",
    "                posts_data.append(comment_info)\n",
    "                number += 1\n",
    "\n",
    "            if number >= 5000:\n",
    "                df = pd.DataFrame(posts_data)\n",
    "                csv_filename = f\"{subreddit_name}_data_partial_{int(time.time())}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                print(f\"Data scraped and saved to {csv_filename}\")\n",
    "\n",
    "                print(number, \"Could I Continue?\")\n",
    "                response = input().strip().upper()\n",
    "                if response == \"N\":\n",
    "                    return posts_data, after_timestamp\n",
    "                number = 0\n",
    "\n",
    "            after_timestamp = submission.created_utc\n",
    "            time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    return posts_data, after_timestamp\n",
    "\n",
    "data, last_timestamp = scrape_subreddit(subreddit_name)\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = f\"{subreddit_name}_data_final_{int(last_timestamp)}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data scraped and saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3740645",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(number)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m posts_data, after_timestamp\n\u001b[1;32m---> 70\u001b[0m data, last_timestamp \u001b[38;5;241m=\u001b[39m scrape_subreddit(subreddit_name)\n\u001b[0;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     72\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubreddit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(last_timestamp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 65\u001b[0m, in \u001b[0;36mscrape_subreddit\u001b[1;34m(subreddit_name)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData scraped and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m     after_timestamp \u001b[38;5;241m=\u001b[39m submission\u001b[38;5;241m.\u001b[39mcreated_utc\n\u001b[1;32m---> 65\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Sleep to avoid hitting API rate limits\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(number)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m posts_data, after_timestamp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Reddit API Credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    ")\n",
    "\n",
    "subreddit_name = \"ChatGPT\"  # Subreddit to scrape\n",
    "\n",
    "def scrape_subreddit(subreddit_name):\n",
    "    posts_data = []\n",
    "    after_timestamp = None\n",
    "    number = 0\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.new(limit=None):\n",
    "        if after_timestamp and submission.created_utc >= after_timestamp:\n",
    "            continue\n",
    "\n",
    "        post_info = {\n",
    "            \"Title\": submission.title,\n",
    "            \"Text\": submission.selftext,\n",
    "            \"Timestamp\": datetime.datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "            \"Author\": submission.author.name if submission.author else None,\n",
    "            \"Upvotes\": submission.score,\n",
    "            \"Number of Comments\": submission.num_comments,\n",
    "            \"URL\": submission.url\n",
    "        }\n",
    "        posts_data.append(post_info)\n",
    "        number += 1\n",
    "\n",
    "        # Scrape comments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comment_info = {\n",
    "                \"Title\": \"Comment\",\n",
    "                \"Text\": comment.body,\n",
    "                \"Timestamp\": datetime.datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                \"Author\": comment.author.name if comment.author else None,\n",
    "                \"Upvotes\": comment.score,\n",
    "                \"Number of Comments\": \"Comment\",\n",
    "                \"URL\": submission.url\n",
    "            }\n",
    "            posts_data.append(comment_info)\n",
    "            number += 1\n",
    "\n",
    "        if number % 5000 == 0:  # Check if number is a multiple of 5000\n",
    "            print(f\"Scraped {number} data items.\")\n",
    "            df = pd.DataFrame(posts_data)\n",
    "            csv_filename = f\"{subreddit_name}_data_{number}.csv\"\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Data scraped and saved to {csv_filename}\")\n",
    "\n",
    "        after_timestamp = submission.created_utc\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "    \n",
    "    print(number)\n",
    "    return posts_data, after_timestamp\n",
    "\n",
    "data, last_timestamp = scrape_subreddit(subreddit_name)\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = f\"{subreddit_name}_data_{int(last_timestamp)}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data scraped and saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea59ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 5001 data items.\n",
      "Data scraped and saved to worldnews_data_5001.csv\n",
      "Scraped 10067 data items.\n",
      "Data scraped and saved to worldnews_data_10067.csv\n",
      "Scraped 15003 data items.\n",
      "Data scraped and saved to worldnews_data_15003.csv\n",
      "Scraped 24763 data items.\n",
      "Data scraped and saved to worldnews_data_24763.csv\n",
      "Scraped 25010 data items.\n",
      "Data scraped and saved to worldnews_data_25010.csv\n"
     ]
    },
    {
     "ename": "TooManyRequests",
     "evalue": "received 429 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Sleep to avoid hitting API rate limits\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m posts_data\n\u001b[1;32m---> 69\u001b[0m data \u001b[38;5;241m=\u001b[39m scrape_subreddit(subreddit_name)\n\u001b[0;32m     70\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     71\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubreddit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[26], line 41\u001b[0m, in \u001b[0;36mscrape_subreddit\u001b[1;34m(subreddit_name)\u001b[0m\n\u001b[0;32m     38\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Scrape comments\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m submission\u001b[38;5;241m.\u001b[39mcomments\u001b[38;5;241m.\u001b[39mreplace_more(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m submission\u001b[38;5;241m.\u001b[39mcomments\u001b[38;5;241m.\u001b[39mlist():\n\u001b[0;32m     43\u001b[0m     comment_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m: comment\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m\"\u001b[39m: submission\u001b[38;5;241m.\u001b[39murl\n\u001b[0;32m     51\u001b[0m     }\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\models\\comment_forest.py:183\u001b[0m, in \u001b[0;36mCommentForest.replace_more\u001b[1;34m(self, limit, threshold)\u001b[0m\n\u001b[0;32m    180\u001b[0m     item\u001b[38;5;241m.\u001b[39m_remove_from\u001b[38;5;241m.\u001b[39mremove(item)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m new_comments \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mcomments(update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\models\\reddit\\more.py:76\u001b[0m, in \u001b[0;36mMoreComments.comments\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease file a bug report with PRAW.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren),\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmission\u001b[38;5;241m.\u001b[39mfullname,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmission\u001b[38;5;241m.\u001b[39mcomment_sort,\n\u001b[0;32m     75\u001b[0m }\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reddit\u001b[38;5;241m.\u001b[39mpost(API_PATH[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmorechildren\u001b[39m\u001b[38;5;124m\"\u001b[39m], data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comments:\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\reddit.py:842\u001b[0m, in \u001b[0;36mReddit.post\u001b[1;34m(self, path, data, files, json, params)\u001b[0m\n\u001b[0;32m    840\u001b[0m attempts \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objectify_request(\n\u001b[0;32m    843\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    844\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    845\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    846\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    847\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    848\u001b[0m         path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RedditAPIException \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    851\u001b[0m     last_exception \u001b[38;5;241m=\u001b[39m exception\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\reddit.py:517\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_objectify_request\u001b[39m(\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objector\u001b[38;5;241m.\u001b[39mobjectify(\n\u001b[1;32m--> 517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    518\u001b[0m             data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    519\u001b[0m             files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    520\u001b[0m             json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    521\u001b[0m             method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    522\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    523\u001b[0m             path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    524\u001b[0m         )\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\util\\deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     37\u001b[0m     warn(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\praw\\reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_core\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    942\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    943\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    944\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    945\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    946\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    947\u001b[0m         path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_with_retries(\n\u001b[0;32m    329\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    330\u001b[0m     files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    331\u001b[0m     json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    332\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    333\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    334\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    335\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    336\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\prawcore\\sessions.py:267\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_retry(\n\u001b[0;32m    255\u001b[0m         data,\n\u001b[0;32m    256\u001b[0m         files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m         url,\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: received 429 HTTP response"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Reddit API Credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "subreddit_name = \"worldnews\"  # Subreddit to scrape\n",
    "\n",
    "def scrape_subreddit(subreddit_name):\n",
    "    posts_data = []\n",
    "    number = 0\n",
    "    last_saved_number = 0\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.new(limit=None):\n",
    "        post_info = {\n",
    "            \"Title\": submission.title,\n",
    "            \"Text\": submission.selftext,\n",
    "            \"Timestamp\": datetime.datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "            \"Author\": submission.author.name if submission.author else None,\n",
    "            \"Upvotes\": submission.score,\n",
    "            \"Number of Comments\": submission.num_comments,\n",
    "            \"URL\": submission.url\n",
    "        }\n",
    "        posts_data.append(post_info)\n",
    "        number += 1\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        # Scrape comments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comment_info = {\n",
    "                \"Title\": \"Comment\",\n",
    "                \"Text\": comment.body,\n",
    "                \"Timestamp\": datetime.datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                \"Author\": comment.author.name if comment.author else None,\n",
    "                \"Upvotes\": comment.score,\n",
    "                \"Number of Comments\": \"Comment\",\n",
    "                \"URL\": submission.url\n",
    "            }\n",
    "            posts_data.append(comment_info)\n",
    "            number += 1\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        # Check if number has exceeded a multiple of 5000 since the last save\n",
    "        if number // 5000 > last_saved_number // 5000:\n",
    "            print(f\"Scraped {number} data items.\")\n",
    "            df = pd.DataFrame(posts_data)\n",
    "            csv_filename = f\"{subreddit_name}_data_{number}.csv\"\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Data scraped and saved to {csv_filename}\")\n",
    "            last_saved_number = number\n",
    "\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "data = scrape_subreddit(subreddit_name)\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = f\"{subreddit_name}_data_final.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data scraped and saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75756a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 6649 data items.\n",
      "Data scraped and saved to worldnews_data_6649.csv\n",
      "Scraped 10183 data items.\n",
      "Data scraped and saved to worldnews_data_10183.csv\n",
      "Scraped 16437 data items.\n",
      "Data scraped and saved to worldnews_data_16437.csv\n",
      "Scraped 20180 data items.\n",
      "Data scraped and saved to worldnews_data_20180.csv\n",
      "Scraped 28492 data items.\n",
      "Data scraped and saved to worldnews_data_28492.csv\n",
      "Scraped 30062 data items.\n",
      "Data scraped and saved to worldnews_data_30062.csv\n",
      "Scraped 35034 data items.\n",
      "Data scraped and saved to worldnews_data_35034.csv\n",
      "Scraped 40710 data items.\n",
      "Data scraped and saved to worldnews_data_40710.csv\n",
      "Scraped 45147 data items.\n",
      "Data scraped and saved to worldnews_data_45147.csv\n"
     ]
    }
   ],
   "source": [
    "#!pip install praw\n",
    "\n",
    "# GUIDE\n",
    "# https://www.geeksforgeeks.org/how-to-get-client_id-and-client_secret-for-python-reddit-api-registration/\n",
    "# https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example\n",
    "\n",
    "# ACCOUNT\n",
    "# yijiebai@126.com\n",
    "# Doudou001\n",
    "# id: yijiebai2000\n",
    "\n",
    "# APPLICATION\n",
    "# name: Yijie's_bot\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Reddit API Credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "subreddit_name = \"worldnews\"  # Subreddit to scrape\n",
    "\n",
    "def scrape_subreddit(subreddit_name):\n",
    "    posts_data = []\n",
    "    number = 0\n",
    "    last_saved_number = 0\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.new(limit=None):\n",
    "        post_info = {\n",
    "            \"Title\": submission.title,\n",
    "            \"Text\": submission.selftext,\n",
    "            \"Timestamp\": datetime.datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "            \"Author\": submission.author.name if submission.author else None,\n",
    "            \"Upvotes\": submission.score,\n",
    "            \"Number of Comments\": submission.num_comments,\n",
    "            \"URL\": submission.url\n",
    "        }\n",
    "        posts_data.append(post_info)\n",
    "        number += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Scrape comments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comment_info = {\n",
    "                \"Title\": \"Comment\",\n",
    "                \"Text\": comment.body,\n",
    "                \"Timestamp\": datetime.datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                \"Author\": comment.author.name if comment.author else None,\n",
    "                \"Upvotes\": comment.score,\n",
    "                \"Number of Comments\": \"Comment\",\n",
    "                \"URL\": submission.url\n",
    "            }\n",
    "            posts_data.append(comment_info)\n",
    "            number += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Check if number has exceeded a multiple of 5000 since the last save\n",
    "        if number // 5000 > last_saved_number // 5000:\n",
    "            print(f\"Scraped {number} data items.\")\n",
    "            df = pd.DataFrame(posts_data)\n",
    "            csv_filename = f\"{subreddit_name}_data_{number}.csv\"\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Data scraped and saved to {csv_filename}\")\n",
    "            last_saved_number = number\n",
    "\n",
    "        time.sleep(15)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "data = scrape_subreddit(subreddit_name)\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = f\"{subreddit_name}_data_final.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data scraped and saved to {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be5fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 5135 data items.\n",
      "Data scraped and saved to worldnews_data_5135.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Sleep to avoid hitting API rate limits\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m posts_data\n\u001b[1;32m---> 83\u001b[0m data \u001b[38;5;241m=\u001b[39m scrape_subreddit(subreddit_name)\n\u001b[0;32m     84\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     85\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubreddit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 68\u001b[0m, in \u001b[0;36mscrape_subreddit\u001b[1;34m(subreddit_name)\u001b[0m\n\u001b[0;32m     66\u001b[0m     posts_data\u001b[38;5;241m.\u001b[39mappend(comment_info)\n\u001b[0;32m     67\u001b[0m     number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 68\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Check if number has exceeded a multiple of 5000 since the last save\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5000\u001b[39m \u001b[38;5;241m>\u001b[39m last_saved_number \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5000\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!pip install praw\n",
    "\n",
    "# GUIDE\n",
    "# https://www.geeksforgeeks.org/how-to-get-client_id-and-client_secret-for-python-reddit-api-registration/\n",
    "# https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example\n",
    "\n",
    "# ACCOUNT\n",
    "# yijiebai@126.com\n",
    "# Doudou001\n",
    "# id: yijiebai2000\n",
    "\n",
    "# APPLICATION\n",
    "# name: Yijie's_bot\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Reddit API Credentials\n",
    "client_id = '8e04u70nKE0-ngexk9BaCg'\n",
    "client_secret = 'froUzXFs2j-QAUYiJFBInQigeYOH3A'\n",
    "user_agent = 'Yijie_bot'\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "subreddit_name = \"worldnews\"  # Subreddit to scrape\n",
    "\n",
    "def scrape_subreddit(subreddit_name):\n",
    "    posts_data = []\n",
    "    number = 0\n",
    "    last_saved_number = 0\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.new(limit=None):\n",
    "        post_info = {\n",
    "            \"Title\": submission.title,\n",
    "            \"Text\": submission.selftext,\n",
    "            \"Timestamp\": datetime.datetime.fromtimestamp(submission.created_utc).isoformat(),\n",
    "            \"Author\": submission.author.name if submission.author else None,\n",
    "            \"Upvotes\": submission.score,\n",
    "            \"Number of Comments\": submission.num_comments,\n",
    "            \"URL\": submission.url\n",
    "        }\n",
    "        posts_data.append(post_info)\n",
    "        number += 1\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Scrape comments\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            comment_info = {\n",
    "                \"Title\": \"Comment\",\n",
    "                \"Text\": comment.body,\n",
    "                \"Timestamp\": datetime.datetime.fromtimestamp(comment.created_utc).isoformat(),\n",
    "                \"Author\": comment.author.name if comment.author else None,\n",
    "                \"Upvotes\": comment.score,\n",
    "                \"Number of Comments\": \"Comment\",\n",
    "                \"URL\": submission.url\n",
    "            }\n",
    "            posts_data.append(comment_info)\n",
    "            number += 1\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Check if number has exceeded a multiple of 5000 since the last save\n",
    "        if number // 5000 > last_saved_number // 5000:\n",
    "            print(f\"Scraped {number} data items.\")\n",
    "            df = pd.DataFrame(posts_data)\n",
    "            csv_filename = f\"{subreddit_name}_data_{number}.csv\"\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Data scraped and saved to {csv_filename}\")\n",
    "            last_saved_number = number\n",
    "\n",
    "        time.sleep(1)  # Sleep to avoid hitting API rate limits\n",
    "\n",
    "    return posts_data\n",
    "\n",
    "data = scrape_subreddit(subreddit_name)\n",
    "df = pd.DataFrame(data)\n",
    "csv_filename = f\"{subreddit_name}_data_final.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data scraped and saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d24ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
